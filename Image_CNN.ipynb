{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upZqt-1uKVq3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqF8ZcqdK5Dp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_dir = '/content/drive/MyDrive/train/'\n",
        "valid_dir = '/content/drive/MyDrive/valid/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zX4o9m3BJEdx"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1VxW3urJHGx"
      },
      "outputs": [],
      "source": [
        "# get images\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    seed=123,\n",
        "    # shuffle=True,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    valid_dir,\n",
        "    seed=123,\n",
        "    # shuffle=True,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     train_dir,\n",
        "#     seed=123,\n",
        "#     shuffle=True,\n",
        "#     validation_split=0.2,\n",
        "#     subset=\"training\",\n",
        "#     image_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "# )\n",
        "# valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     train_dir,\n",
        "#     seed=123,\n",
        "#     shuffle=True,\n",
        "#     validation_split=0.2,\n",
        "#     subset=\"validation\",\n",
        "#     image_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "# )\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ad0yFN8IW6pF"
      },
      "outputs": [],
      "source": [
        "# INCEPTION MODEL\n",
        "def inception_model():\n",
        "  input = tf.keras.Input(shape=(224, 224, 3))\n",
        "  intermediate = tf.keras.layers.Rescaling(1./255)(input)\n",
        "\n",
        "  # inception\n",
        "  left = layers.Conv2D(32, 1, padding='same', activation='relu')(intermediate)\n",
        "  middle_input = layers.Conv2D(32, 1, padding='same', activation='relu')(intermediate)\n",
        "  middle = layers.Conv2D(32, 3, padding='same', activation='relu')(middle_input)\n",
        "  right_input = layers.Conv2D(32, 1, padding='same', activation='relu')(intermediate)\n",
        "  right_input = layers.Conv2D(32, 3, padding='same', activation='relu')(right_input)\n",
        "  right = layers.Conv2D(32, 5, padding='same', activation='relu')(right_input)\n",
        "  intermediate = layers.concatenate([left, middle, right], axis = 3)\n",
        "\n",
        "  intermediate = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "\n",
        "  left = layers.Conv2D(64, 1, padding='same', activation='relu')(intermediate)\n",
        "  middle_input = layers.Conv2D(128, 1, padding='same', activation='relu')(intermediate)\n",
        "  middle = layers.Conv2D(64, 3, padding='same', activation='relu')(middle_input)\n",
        "  right_input = layers.Conv2D(256, 1, padding='same', activation='relu')(intermediate)\n",
        "  right_input = layers.Conv2D(128, 3, padding='same', activation='relu')(right_input)\n",
        "  right = layers.Conv2D(64, 5, padding='same', activation='relu')(right_input)\n",
        "  intermediate = layers.concatenate([left, middle, right], axis = 3)\n",
        "\n",
        "  # intermediate = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(intermediate)\n",
        "  # intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  # intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "\n",
        "  # end\n",
        "  intermediate = tf.keras.layers.Flatten()(intermediate)\n",
        "  # intermediate = tf.keras.layers.Dense(1024)(intermediate)\n",
        "  # intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  # intermediate = tf.keras.layers.Dropout(0.5)(intermediate)\n",
        "  output = tf.keras.layers.Dense(11)(intermediate)\n",
        "\n",
        "  # model\n",
        "  model = tf.keras.Model(input, output)\n",
        "  optimizer = tf.keras.optimizers.SGD()\n",
        "  optimizer.momentum = 0.75\n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  epochs = 20\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=epochs\n",
        "  )\n",
        "\n",
        "  # plotting\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs_range = range(epochs)\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.show()\n",
        "# inception_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lAly_vSlKyOc"
      },
      "outputs": [],
      "source": [
        "def residual(input):\n",
        "  output = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(input)\n",
        "  output = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(output)\n",
        "  out = tf.keras.layers.Add()([input, output])\n",
        "  return out\n",
        "\n",
        "def residual2(input):\n",
        "  output = tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(input)\n",
        "  output = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(output)\n",
        "  out = tf.keras.layers.Add()([input, output])\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YwFfvc9wM-_h"
      },
      "outputs": [],
      "source": [
        "# RESIDUAL CONNECTIONS MODEL\n",
        "# Background knowledge and inspiration taken from https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
        "\n",
        "def residual_model():\n",
        "  # starting\n",
        "  input = tf.keras.Input(shape=(224, 224, 3))\n",
        "  intermediate = tf.keras.layers.Rescaling(1./255)(input)\n",
        "  intermediate = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "  intermediate = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "\n",
        "  # residual\n",
        "  intermediate = residual(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "  intermediate = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "  intermediate = residual2(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.MaxPooling2D()(intermediate)\n",
        "\n",
        "  # end\n",
        "  intermediate = tf.keras.layers.Flatten()(intermediate)\n",
        "  intermediate = tf.keras.layers.Dense(1024, activation='relu')(intermediate)\n",
        "  intermediate = tf.keras.layers.BatchNormalization()(intermediate)\n",
        "  intermediate = tf.keras.layers.Dropout(0.5)(intermediate)\n",
        "  output = tf.keras.layers.Dense(11)(intermediate)\n",
        "\n",
        "  # model\n",
        "  model = tf.keras.Model(input, output)\n",
        "  optimizer = tf.keras.optimizers.SGD()\n",
        "  optimizer.momentum = 0.75\n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  epochs = 20\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=epochs\n",
        "  )\n",
        "\n",
        "  # plotting\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs_range = range(epochs)\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.show()\n",
        "# residual_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "shnLaydkiXGp"
      },
      "outputs": [],
      "source": [
        "# BEST MODEL\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "num_classes = 11\n",
        "initializer = tf.keras.initializers.GlorotNormal(seed=123)\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(128, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(256, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(512, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(2048, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(1024, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(num_classes, kernel_initializer=initializer),\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "optimizer.momentum = 0.75\n",
        "model.compile(\n",
        "  optimizer=optimizer,\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WUW6bwQETkn"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "# plotting\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('\\n--- SUMMARY ---')\n",
        "print(\"Max Acc - Train:\", max(acc), \" Valid:\", max(val_acc))\n",
        "print(\"Min Loss - Train:\", min(loss), \" Valid:\", min(val_loss))\n",
        "\n",
        "# plotting\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uzdpeZZIyuw"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "checkpoint_path = \"/content/drive/MyDrive/savedmodel/training_model.ckpt\"\n",
        "# Create a callback that saves the model's weights for best model (best = highest train accuracy)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_best_model=True)\n",
        "\n",
        "# data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  # layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  # layers.RandomContrast(factor=0.05, seed=123),\n",
        "])\n",
        "train_ds_1 = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "history = model.fit(\n",
        "  train_ds_1,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('\\n--- SUMMARY ---')\n",
        "print(\"Max Acc - Train:\", max(acc), \" Valid:\", max(val_acc))\n",
        "print(\"Min Loss - Train:\", min(loss), \" Valid:\", min(val_loss))\n",
        "\n",
        "# data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  # layers.RandomContrast(factor=0.05, seed=123),\n",
        "])\n",
        "train_ds_1 = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "history = model.fit(\n",
        "  train_ds_1,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('\\n--- SUMMARY ---')\n",
        "print(\"Max Acc - Train:\", max(acc), \" Valid:\", max(val_acc))\n",
        "print(\"Min Loss - Train:\", min(loss), \" Valid:\", min(val_loss))\n",
        "    \n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomContrast(factor=0.05, seed=123),\n",
        "])\n",
        "train_ds_1 = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "history = model.fit(\n",
        "  train_ds_1,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('\\n--- SUMMARY ---')\n",
        "print(\"Max Acc - Train:\", max(acc), \" Valid:\", max(val_acc))\n",
        "print(\"Min Loss - Train:\", min(loss), \" Valid:\", min(val_loss))\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomContrast(factor=0.1, seed=123),\n",
        "])\n",
        "# SAVE WEIGHTS\n",
        "train_ds_2 = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "history = model.fit(\n",
        "  train_ds_2,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=epochs,\n",
        "  callbacks=[cp_callback]\n",
        ")\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('\\n--- SUMMARY ---')\n",
        "print(\"Max Acc - Train:\", max(acc), \" Valid:\", max(val_acc))\n",
        "print(\"Min Loss - Train:\", min(loss), \" Valid:\", min(val_loss))\n",
        "\n",
        "# plotting\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kgSGOAfvI-97"
      },
      "outputs": [],
      "source": [
        "### --- TESTING --- ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-tQcd3IcC_E"
      },
      "outputs": [],
      "source": [
        "# --- TEST SCRIPT CELL ---\n",
        "import json\n",
        "\n",
        "# model\n",
        "num_classes = 11\n",
        "class_names = ['creamy_paste', 'diced', 'floured', 'grated', 'juiced', 'jullienne', 'mixed', 'other', 'peeled', 'sliced', 'whole']\n",
        "initializer = tf.keras.initializers.GlorotNormal(seed=123)\n",
        "\n",
        "# MODEL\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(128, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(256, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(512, 3, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(2048, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(1024, activation='relu', kernel_initializer=initializer),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(num_classes, kernel_initializer=initializer),\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "optimizer.momentum = 0.75\n",
        "model.compile(\n",
        "  optimizer=optimizer,\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "# load model weights\n",
        "checkpoint_path = \"/content/drive/MyDrive/savedmodel/training_model.ckpt\"\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "# testing\n",
        "dictionary = {}\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "list_images = glob.glob(\"/content/drive/MyDrive/test/anonymous/*.jpg\")\n",
        "for image_path in list_images:\n",
        "    items = image_path.split('/')\n",
        "    image_name = items[-1]\n",
        "    img = tf.keras.utils.load_img(\n",
        "        path=image_path, target_size=(img_height, img_width)\n",
        "    )\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "    prediction_class = class_names[np.argmax(score)]\n",
        "    dictionary[image_name] = prediction_class\n",
        "\n",
        "# Writing to prediction.json file\n",
        "json_object = json.dumps(dictionary, indent=4)\n",
        "with open(\"/content/drive/MyDrive/prediction.json\", \"w+\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
