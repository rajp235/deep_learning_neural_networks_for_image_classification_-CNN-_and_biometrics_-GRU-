{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upZqt-1uKVq3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqF8ZcqdK5Dp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/training-validation/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3swtVueLaUw"
      },
      "outputs": [],
      "source": [
        "# list directories\n",
        "!ls \"$data_dir\"\n",
        "# counter number of files\n",
        "!find \"$data_dir\" -name '*.csv' -type f | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xMhDOsLBMpu3"
      },
      "outputs": [],
      "source": [
        "csv_files = glob.glob(data_dir + \"/**/*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "me2Dt8xCJBOL"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for f in csv_files:\n",
        "  write = True\n",
        "  data = pd.read_csv(f, header=None)\n",
        "  del data[data.columns[0]]\n",
        "  data = data.values.tolist()\n",
        "  for i in range(len(data)):\n",
        "    instance = data[i]\n",
        "    if instance[3] == \"#\":\n",
        "      write = False\n",
        "      break\n",
        "    if instance[2] == \"--1\":\n",
        "      write = False\n",
        "      break\n",
        "  if write:\n",
        "    data_list.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_L9NYLL2UYZd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, valid = train_test_split(data_list, test_size=0.1)\n",
        "train_x = train\n",
        "train_y = []\n",
        "for i in range(len(train_x)):\n",
        "  appending = True\n",
        "  for j in range(len(train_x[i])):\n",
        "    instance = train_x[i][j]\n",
        "    last_value = instance.pop()\n",
        "    if appending:\n",
        "      train_y.append(last_value)\n",
        "      appending = False\n",
        "    train_x[i][j] = instance\n",
        "valid_x = valid\n",
        "valid_y = []\n",
        "for i in range(len(valid_x)):\n",
        "  appending = True\n",
        "  for j in range(len(valid_x[i])):\n",
        "    instance = valid_x[i][j]\n",
        "    last_value = instance.pop()\n",
        "    if appending:\n",
        "      valid_y.append(last_value)\n",
        "      appending = False\n",
        "    valid_x[i][j] = instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GhB-jOHPxLlE"
      },
      "outputs": [],
      "source": [
        "# change to tensors\n",
        "train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "valid_x = tf.convert_to_tensor(valid_x, dtype=tf.float32)\n",
        "valid_y = tf.convert_to_tensor(valid_y, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DswvdQPaWP_X"
      },
      "outputs": [],
      "source": [
        "# --- MODEL ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8DvdU89yX3H9"
      },
      "outputs": [],
      "source": [
        "# GRU NETWORK\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=123)\n",
        "# initializer = tf.keras.initializers.RandomUniform(seed=123)\n",
        "# initializer = tf.keras.initializers.GlorotNormal(seed=123)\n",
        "# reg = tf.keras.regulizers.L1()\n",
        "# reg = tf.keras.regularizers.L1L2()\n",
        "\n",
        "# GRU NETWORK\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.GRU(units=60, return_sequences = True, kernel_initializer=initializer),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.GRU(units=60, return_sequences = True, kernel_initializer=initializer),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.GRU(units=60, kernel_initializer=initializer),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])\n",
        "\n",
        "# parameters\n",
        "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "# opt = tf.keras.optimizers.RMSprop()\n",
        "# opt = tf.keras.optimizers.Adamax()\n",
        "# opt = tf.keras.optimizers.Adagrad()\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "# batch_size_val = 5\n",
        "# batch_size_val = 10\n",
        "batch_size_val = 15\n",
        "# batch_size_val = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJM2Lw91TM3x"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "checkpoint_path = \"/content/drive/MyDrive/savedmodel/training_model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights for best model (best = highest train accuracy)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_best_model=True)\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "epochs = 100\n",
        "history = model.fit(train_x, train_y, \n",
        "                    batch_size = batch_size_val,\n",
        "                    epochs = epochs,\n",
        "                    callbacks=[cp_callback],\n",
        "                    validation_data=(valid_x,valid_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKMw-JpuTnmP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ZQOggfYQTGjs"
      },
      "outputs": [],
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxTL-ABiTJQN"
      },
      "outputs": [],
      "source": [
        "# --- TEST SCRIPT CELL ---\n",
        "import json\n",
        "\n",
        "# model\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=123)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.GRU(units=60, return_sequences = True, kernel_initializer=initializer),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.GRU(units=60, return_sequences = True, kernel_initializer=initializer),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.GRU(units=60, kernel_initializer=initializer),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "batch_size_val = 15\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# load model weights\n",
        "checkpoint_path = \"/content/drive/MyDrive/savedmodel/training_model.ckpt\"\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "# testing\n",
        "dictionary = {}\n",
        "test_directory = \"/content/drive/MyDrive/training-validation\"\n",
        "list_files = csv_files = glob.glob(test_directory + \"/**/*.csv\")\n",
        "class_names = [0, 1, 2]\n",
        "\n",
        "data_list = []\n",
        "for file in csv_files:\n",
        "  write = True\n",
        "  file_name = file.split('/')[-1]\n",
        "  data = pd.read_csv(file, header=None)\n",
        "  del data[data.columns[0]]\n",
        "  data = data.values.tolist()\n",
        "  for i in range(len(data)):\n",
        "    instance = data[i]\n",
        "    if instance[3] == \"#\":\n",
        "      print(\"Skipping file as contains unsupported character\")\n",
        "      write = False\n",
        "      break\n",
        "    if instance[2] == \"--1\":\n",
        "      print(\"Skipping file as contains unsupported character\")\n",
        "      write = False\n",
        "      break\n",
        "  if write:\n",
        "      for j in range(len(data)):\n",
        "        # if testing data contains last column as in training dataset\n",
        "        instance = data[j]\n",
        "        last_value = instance.pop()\n",
        "        data[j] = instance\n",
        "      data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
        "      data = tf.expand_dims(data, 0)\n",
        "      predictions = model.predict(data)\n",
        "      score = tf.nn.softmax(predictions[0])\n",
        "      prediction_class = class_names[np.argmax(score)]\n",
        "      prediction_class_t = predictions.argmax(axis=-1)\n",
        "      dictionary[file_name] = prediction_class\n",
        "\n",
        "# Writing to prediction.json file\n",
        "json_object = json.dumps(dictionary, indent=2)\n",
        "with open(\"/content/drive/MyDrive/prediction.json\", \"w+\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
